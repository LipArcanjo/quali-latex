\section{Study with University Students}

As presented in Chapter \ref{cha:method}. It was proposed to students, as one of the activities 
of the Free Software course, that they make the first-time contribution to the Linux kernel to 
mitigate duplication found by the ArKanjo tool while sending patches to the Linux subsystem 
maintainers and documenting their experiences and learnings.

Appendix \ref{app:stuart} has references to the artifacts discussed in this section, 
including the duplication function pairs that students mitigate, the studentsâ€™ reports, 
the patch students sent to the Linux kernel, and the answers to the survey asked 
of the student who worked in the AMD Display driver and the teaching assistant that used 
the tool to find duplications in the IIO subsystem.

\input{conteudo/results/table_students}

Table \ref{tab:stu} summarizes the student groups' mitigation approaches. It contains 
the information about the chosen subsystem and the similarity threshold for groups that 
opted for the first approach. Each entry also has the patch status, refactoring method, 
and code diff corresponding to the final patch versions before it was dropped or accepted 
into the code base. The code diff for a patchset corresponds to the sum of added and removed 
lines of each of its patches.

Seven groups applied the parameterized refactor method, two groups used the extractor method, and one group applied the inline method to their approaches to mitigate their duplications. Four patches do not contain a refactoring technique defined in the literature (undefined). Six groups had their patches accepted and fully merged into Linux; four groups had their patches dropped; one group has its patchset partially applied and is still working on patch submission, currently in the sixth version of the patch when this document was written, but we can also consider this case as accepted.

Groups 1 and 4 have worked in similar duplications, where two functions are involved: one to determine if a register is volatile and the other to determine if it is writable. These are two functions that return opposite Boolean results. Group 1 initially addressed the problem using the extract method, while Group 2 used macros. After feedback from maintainers, both solutions converged to keep one of the functions while changing the content of the other to return a negated query of the former function. However, by the end of the feedback cycle, maintainers considered the patch from Group 1 irrelevant, while the patch from Group 4 was accepted. The reason is that Group 1 has chosen to apply the refactoring to a device that contains a register that is both read-only and non-volatile. Thus, the technical debt selected by Group 1 was not merely a duplication but an incorrect implementation of the method.

Groups 3, 6, and 10 created a struct to encapsulate the parameters and mitigate duplication when applying the parameterized method. The patches from Groups 6 and 10 were rejected by the same maintainer who approved the patch from Group 3. The maintainer's justification for rejecting the patch from Group 6 is that it proposes to fix duplication while adding more lines than removing, and is not convincing enough in other aspects, such as readability. The reason for rejecting the patch from Group 10 is that they refactored code files with simple context; therefore, adding a layer of abstraction to remove duplication decreases code readability without clear gains in codebase quality. The maintainers' feedback for Group 10 was:
\begin{quote}

\textit{``A few comments inline, but overall I'm not sure the code reduction is sufficient to cover the resulting loss of readability. Sometimes a switch is simply clearer than a partial look-up table.''}

\textit{``So nice idea, but I'm not seeing it as being a good move for this particular code. I'd be slightly interested to see the optimized output of the two approaches, but this is far from a high-performance path, so we care a lot more about readability here.''}
\end{quote}

Groups 7 and 8 used the extract method to approach the mitigations. Group 8 sent the mitigations to the maintainers, and the code changes were approved, with minor requested changes in code style and smaller errors fixed. For Group 7 mitigation, the maintainer initially rejected the proposed patch but suggested a new refactoring approach: applying the extraction method refactoring to the duplicated code to an existing helper function. After the feedback cycle, maintainers accepted the patch in version 3.

Group 9 initially approached the selected duplication with a parameterized method, but after entering the feedback cycle, it was revealed that the problem represented a much more complex technical debt. The patch is still under development, currently at version 6, and has been separated into a 4-patch patch set, where half of them have been applied, and the remaining entries are receiving suggestions. Fortunately, maintainers are receptive and provide constant feedback; the contribution is expected to be fully merged if the students do not abandon the current work.

Group 11 is the student who executed the tool and completed the process of identifying duplications. The student identified a duplication in the driver that occurred across eighteen code files, creating a patch to remove approximately four hundred lines of duplicated code. The student sent the mitigations to the maintainers, and their feedback pointed to fixing the warning in the code without arguing about removing the duplications. After addressing the issue, the patch was applied. This student also contributed to IIO, tackling a code duplication with inline method refactoring. This one, however, was received with more skepticism by the maintainer, and the student dropped the contribution because he did not think the feedback instructions were clear enough.

Analyzing the students' approaches to the duplications and the maintainers' feedback, we saw that people without previous experience in Linux kernel development could approach duplications found by the \texttt{anonymous-tool} and become contributors to the kernel. We observed that not all duplications
are viewed as code of bad quality, with maintainers analyzing the trade-off between the purpose of the duplicated code, code readability, and the levels of abstraction added to address the duplications.

\subsection{Statistics of the contributions}

\todo[inline]{Is it ok to add it here in the text? It was Spessoto that writed this section entirely and I reviewed it}

The eleven groups, which have submitted 11 contributions to the IIO subsystem, have collectively 
requested the insertion of 379 lines and the deletion of 605 lines in the subsystem, 
representing an average of 34.45 lines added and 55 lines removed per contribution.

Considering the sum of fully merged contributions, we have a code diff value of +51/-143, 
with an average of +12.75/-35.75. This ratio is better than dropped contributions, which 
have a total of +179/-219 with an average of +29.83/-36.5.

It is also possible to notice a difference based on the similarity threshold used to 
detect duplications. The proposed patches to address 100\%-similarity duplications 
summed up to +46/-90 (average of +9.2/-18). For a 90\% value, the result was a total 
of +333/-515 with an average of +55.5/-85.83. The 90\% value enabled ArKanjo to identify 
less strict duplication cases, and, therefore, code segments with a greater extension, 
compared to the 100\% similarity entries, which were shorter segments, causing a 
difference in the average number of modified lines.

Finally, the AMD contribution from Group 11, which involves the complete use of the ArKanjo toolchain, 
got a completely different outcome. With a total of 90 lines inserted and 489 lines 
removed, there are some points to be considered for the success and high modification 
extension of the contribution:

\begin{itemize}
    \item The AMD Display subsystem has some self-reported problems regarding overall technical debt;
    \item The AMD Display subsystem has a more centralized code structure and maintainer hierarchy, enabling contributors to tackle code duplications across a broader scope and multiple files, which happened to Group 11's contribution;
    \item The complete control over the process of finding code duplication gave Group 11 the opportunity to identify and reason better over the duplications found by \texttt{anonymous-tool}, enabling a more strategic approach to searching for promising patch possibilities.
\end{itemize}

\subsection{Survey Results Analysis}

We asked the teaching assistant and the students who used the ArKanjo to answer a survey with 
some questions related to the tool experience, such as installation, documentation, facility of 
use, new ideas to improve the tool, etc. Appendix \ref{app:survey} has the complete survey 
with the answers.

Related to the tool installation, neither the teaching assistant nor the students faced issues 
installing on the Debian distributions. However, the teaching assistant attempted to install it 
in the Arch Linux distribution and failed to install it on this Linux distribution. 
Regarding the tool's documentation, the teaching assistant pointed out errors that worsened his 
experience. The student asked for more examples and tutorials on using the tool in text or 
video formats.

On the ArKanjo tool preprocessing step, the student reported problems of disk space usage and 
crash errors, which we knew could happen as the tool iterates and can save information about 
every pair of functions in a codebase, as we did not implement any mechanism to control the 
memory usage. The teaching assistant reported only being able to execute the preprocessor with 
the text similarity method, and the diff method kept running for hours without finishing. 
The teaching assistant's report was unexpected behavior for us, as the text similarity method 
initially looked more complex; in contrast, the \textit{diff} method is a simple counter of equal lines 
with an operation system default command. The teaching assistant report corroborates the 
gensim efficiency \citep{gensim}.

Related to the tool's functionalities, both the student assistant and the student reported 
a missing mechanism to save the tool results on a separate file instead of the terminal output. 
The tool, by default, outputted with colors, as shown in section \ref{subsec:func}, which complicates 
redirecting the terminal output to a file.

We asked for the overall impression of the tool and if it met their expectations. We also asked 
them to give a score of one to five for installation, documentation, preprocessing, and functionalities. 
Both the teaching assistant and the student scored at least four on each mentioned point, thought that 
the tool fulfilled their expectations, and had an overall good experience. Finally, the teaching assistant 
the the student reported features ideas to the tool that may be interesting, which 
are showed below.

\paragraph{List of Possible Improvements}

\begin{itemize}

\item Enable the possibility of having several codebases of preprocessor artifacts and selecting one of them.

\item Add different methods to find duplication.

\item Adding more filters to search for duplicates, such as finding duplicates with more than 90\% 
similarity and more than 20 lines repeated.

\item Add options to highlight and filter duplications on the same file or directory.

\item Addition of support to save the tool output in files without manual preprocessing of the colored output.

\end{itemize}

With the survey feedback, we understood that users had a good experience and found value in the 
ArKanjo tool. They also provided critical points that could be improved. The most critical 
reported issues, such as documentation errors and a lack of support for multiple Linux distributions 
and different operating systems, have been fixed. Less critical issues remain open points that 
can be addressed in future work.

\subsection{Students Contribution to the Tool}

We listed multiple issues in the tool for the two groups that opted to contribute directly 
to developing the ArKanjo tool. The first group opted to work on expanding the tool's support 
for multiple operating systems. Some libraries used in the tool development only supported the 
Ubuntu distribution, and the first group was responsible for migrating these libraries to 
libraries with support for multiple operating systems. This issue was of our knowledge and 
was pointed out by the teaching assistant in the survey, as he reported the problem while 
using the tool.

The second group opted to work on a webpage for the tool documentation and support for 
light and dark modes. The colored output presented in the figures in Chapter \ref{cha:tool} was 
hard coded at first, not considering the user terminal colors, thus giving a bad user experience. 
The website helps facilitate people's contribution or use of the tool. We did not know how to 
approach the light and dark modes at first and did not need to look at how to fix the issue as 
the group searched and found a solution.

The students successfully delivered the issues proposed. We found it interesting to see 
people able to contribute to the tool on the problems that we did not even know how to approach.
The issues help the user experience and facilitate other research using the ArKanjo tool that 
we may explore in future works. Appendix \ref{app:contribution} references the 
students' contributions, the issue list proposed to the students, and figures from 
the documentation webpage.
