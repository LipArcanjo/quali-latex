
We employ a multi-method research approach, using different methodologies to achieve our 
goals of approaching code duplication in some of the Linux subsystems. To validate if the 
ArKanjo tool can find code duplications, we applied two independent methods and performed 
triangulation on the results.

The first method evaluates the proposed tool using an approach from the literature, 
comparing it against the BigCloneBench dataset~\citep{bigclonebench}. 
The second method is an empirical analysis of a subset of functions within the 
AMD Display driver that our tool identified as duplicates. After completing these two 
methods, we triangulated the findings to analyze the results comprehensively. 
We chose triangulation because relying solely on the literature-based method has 
limitations and may be unreliable~\citep{bigfail,litreview}. 
As a reminder, we have done all research approaches using the text similarity method 
as the code duplication detection method of the tool.

Given the validation of the tool capabilities, we proceed to understand the viability 
of mitigating the code duplication found by the tool. We conducted an ethnographic study 
using a participant-observation methodology to understand its viability. In this study, 
we aim to mitigate a subset of duplicated functions by refactoring them while interacting 
with the Linux community to gather feedback on our mitigation efforts.

The ethnographic study had two phases. In the first phase, the author explored the process 
of mitigating a code duplication found in the AMD Display driver and sent it as a PATCH to 
interact with the Linux kernel community and get their feedback. In the second phase, we proposed 
to students in the course Free Software Development at the University of SÃ£o Paulo to mitigate 
the code duplications found by the ArKanjo in the IIO subsystem and the AMD Display driver and 
pass the process of sending the mitigations as a PATCH.

\section{Evaluation of the Tool by the BigCloneBench dataset}
\label{sec:metbig}

The BigCloneBench dataset~\citep{bigclonebench} is a Java code collection containing pairs of code duplications categorized by clone type, as defined in Section~\ref{subsec:types}.
%
Following the methodology presented by~\citep{tailor}, we sampled 20,000 pairs from each clone type, adding 20,000 non-duplicate pairs as negative samples. We applied the same sampling approach to our tool to ensure a fair evaluation.

Unlike traditional methods, our tool does not distinguish between clone types and identifies duplications at the function level rather than the file level, which is typical in state-of-the-art tools. Therefore, we adapted the metric calculation method for evaluation purposes. Specifically, we considered every pair of functions with a similarity metric equal to or greater than a threshold \(X\) as duplicates and marked the corresponding file pairs. A correctly identified duplication pair counts as accurate for its clone type, while an incorrectly inferred pair is considered incorrect across all clone types. With these definitions, we computed the Recall metric from Section~\ref{subsec:codemethods} for each clone type.
%
To understand the impact of varying the similarity threshold, we evaluated our tool using different threshold values: 30\%, 40\%, 50\%, 60\%, 70\%, 80\%, 90\%, and 100\%. We then analyzed and discussed the results.

\section{Evaluation of the Tool by Empirical Analysis}

\label{sec:metemp}

Due to the limitations of the BigCloneBench dataset, as noted by~\cite{bigfail}, and its lack of suitability for our context (Java dataset vs. C-based Linux kernel), we complemented the BigCloneBench evaluation with an empirical study of the tool inferred duplications.

Given the vast number of lines and components in the Linux kernel, analyzing every duplication is impractical. We focused our scope on the AMD Display driver, which, as shown in Figure \ref{fig:relatory_ex}, contains over 20,000 lines of duplicated code at a similarity metric of 100\%.

In this empirical analysis, we randomly sampled function pairs identified by the tool as duplicates. For each similarity threshold \(X\) (30\%, 40\%, 50\%, 60\%, 70\%, 80\%, 90\%, and 100\%), we randomly selected ten function pairs with a similarity close to \(X\), allowing for a 1\% deviation. We implemented the random selection using the tool source code.

Appendix~\ref{app:emp} lists the randomly selected function pairs. We empirically evaluated these pairs to determine if they were duplications, comparing these results with the literature-based method to assess our tool effectiveness and understand the impact of the minimum similarity parameter.

\section{Ethnographic Study}
\label{sec:meteth}

Ethnography is a recognized qualitative empirical method for understanding people, 
their cultures, and their work practices~\citep{bookethno}. It allows researchers 
to gain insights into values, beliefs, and ideas from the perspective of community 
members~\citep{ethnosoft}. In our research, refactoring code duplications without 
community interaction would prevent us from understanding whether Linux maintainers 
would accept our approach or identify their concerns. Thus, we will use ethnography, 
positioning ourselves and students as Linux kernel contributors focused on removing 
identified code duplications.

This section outlines our approach to mitigating code duplication in the Linux kernel 
and describes the ethnographic methodology for engaging with the Linux community. 
Firstly, we present the participant observation research method explored by the author. 
Then, we present the methodology of the non-participant observation analysis of patches 
sent by the students.

\subsection{Participant observation method}
\label{subsec:partmethod}

To investigate whether we can use the ArKanjo tool to find ways to contribute to the 
Linux kernel, we conducted a participant observation experiment in the second semester 
of 2024, where we acted as first-time contributors to the AMD Display driver, collecting 
artifacts of our experience in the process.

In the AMD Display driver codebase, we executed the ArKanjo tool. 
We manually analyzed the biggest duplications regarding the number of lines and found 
two duplicated function pairs that we judged interesting to try to mitigate. We 
have chosen to approach the biggest duplications as proof of concept so that we 
can make significant impacts. 

The first duplicated function pair chosen is function \textit{offset\_to\_id} on files \textit{dc/gpio/dcn32/hw\_translate\_dcn32.c} and \textit{dc/gpio/dcn315/hw\_translate\_dcn315.c}. The second duplicated function pair is function \textit{phy\_id\_to\_atom} in file 
\textit{ \seqsplit{dc/bios/dce110/command\_table\_helper\_dce110.c}} and \textit{\seqsplit{dc/bios/dce60/command\_table\_helper\_dce60.c}}.
Hereafter, the mitigation on \textit{offset\_to\_id} will be referred to as mitigation 1, and the mitigation on 
\textit{phy\_id\_to\_atom} will be referred to as mitigation 2.

Given the two duplicated function pairs, we observed that the files in the functions 
contain multiple other duplications. Thus, we proposed a simple systematic approach 
to mitigate all the functions duplicated in the context, not just the duplicated 
function pairs. 
After we approached the duplications with the systematic strategy, for the duplicated 
function pairs that had successfully mitigated the duplication, we sent patches to the 
AMD Display driver email list to validate the mitigations and see the maintainer's opinion 
while documenting the process and our impressions.
The systematic approach is defined below. 

\subsubsection{Systematic Approach to Mitigate Code Duplication}
\label{subsubsec:systematic}

In C programming, communication between source files is achieved by creating header files that specify 
libraries~\cite{Cbook}. Since the Linux kernel is primarily written in C, we eliminated code duplications by consolidating duplicated code into a single library, which would replace instances of 
duplication across the codebase.

For each duplicated function pair, for each function in the pair, we used the ArKanjo tool to locate all duplications of the function in the codebase, as it may have been duplicated more times than the two occurrences contained in the duplicated function pair. 
This strategy resulted in a collection of duplicated functions and corresponding code files.

To identify shared code more effectively, we extended this approach to search for other common 
functions across all collection files. We then applied specific refactoring methods to each shared 
function. If the functions were identical across files, we removed the duplicates and created a single 
function in the library. If modifications existed, we applied case-specific refactoring.

\subsection{Non-participant Observation Study with University Students}

A core dynamic of the course (concurrently for undergraduate and postgraduate studies) is to 
have students make a practical contribution to a FLOSS project. In the first semester of 
2025, the professor teaching the course, who is also the advisor of this work, identified an 
opportunity to leverage the ArKanjo tool within this existing pedagogical structure. 
As a contribution option, the professor proposed that students mitigate code duplications found 
in the Linux kernel. The professor drove this initiative as a practical course activity, 
and the author of this work was not involved in using the tool to identify duplications or 
in developing approaches to mitigate them. The tasks were carried out entirely by the course 
staff and the students.

The course had 37 students (25 undergraduate and 12 graduate), who were asked to form groups of
two or three members. To assist students in this task, the professor and teaching assistants prepare multiple alternatives for contributing to FLOSS projects, including simpler contributions such as updating documentation or fixing code style issues. In this context, two new options related to the
Arkanjo tool were proposed alongside the others.

For the first ArKanjo-related alternative, a teaching assistant ran the tool on the IIO subsystem twice, creating two lists of duplicated function pairs, one with a 100\% similarity threshold and the other with a 90\% similarity threshold. Then, the following process happened to both lists:

\begin{enumerate}
    \item The teaching assistant filtered the lists to keep only duplicated pairs that happened in the same file, because the IIO subsystem is very decentralized, and a duplication between two different devices may involve the cooperation of different maintainers and/or the unification of totally different interfaces;
    \item The teacher assistant removed duplications that were too short and would not represent a significant change for a patch application;
    \item The teacher assistant manually ranked code duplications and marked some duplications with insights based on personal opinion and perceptions over the remaining entries. This was done to guide students so that they could approach the technical debt with a greater chance of having their contribution merged into the code.
\end{enumerate}

The students' task was to select a recommended entry from the list, refactor the code to eliminate 
these specific duplications, and then submit their patches to the IIO maintainers for review.

For the second alternative, students were offered an experience similar to our initial participant
observation. In this model, the students were responsible for the entire workflow: executing the
ArKanjo on the AMD display driver, independently analyzing the results to identify a duplication
to fix, creating a patch to mitigate it, and sending the patch to the driver maintainers.

Of the students who chose to work on the ArKanjo-related tasks, 23 (16 undergraduate and 7 graduate)
formed 11 groups to pursue the first alternative. One graduate student also opted for the second 
alternative. Notably, none of these students had prior experience contributing to the Linux kernel, 
making this their first attempt to submit a patch as newcomers to the project.

The groups were requested to document their experiences and approaches to refactoring the duplications, as well as their experiences submitting a patch to the Linux kernel, in blogs. We analyzed these blogs to understand the refactoring patterns used to mitigate duplications, the experience of sending patches, and the opinions of subsystem maintainers on the mitigations.
