We employ a multi-method research approach, using different methodologies to achieve our 
goals to approach code duplication in some of the Linux subsystems. To validate if the 
ArKanjo tool can find code duplications, we applied two independent methods and performed 
triangulation on the results.

The first method evaluates the proposed tool using an approach from the literature, 
comparing it against the BigCloneBench dataset \citep{bigclonebench}. 
The second method is an empirical analysis of a subset of functions within the 
AMD Display driver that our tool identified as duplicates. After completing these two 
methods, we triangulated the findings to analyze the results comprehensively. 
We chose triangulation because relying solely on the literature-based method has 
limitations and may may be unreliable \citep{bigfail, litreview}. 
As a reminder, all research approaches were done using the text similarity method 
as the code duplication method of the tool.

Given the validation of the tool’s capabilities, we proceed to understand the viability 
of mitigating the code duplication found by the tool. We conducted an ethnographic study 
using a participant-observation methodology to understand its viability. In this study, 
we aim to mitigate a subset of duplicated functions by refactoring them while interacting 
with the Linux community to gather feedback on our mitigation efforts.

The ethnographic study had two phases. In the first phase, the author explored the process 
of mitigating a code duplication found in the AMD Display driver and sent it as a PATCH to 
interact with the Linux community and get their feedback. In the second phase, we proposed 
to students in the course Free Software Development at the University of São Paulo to mitigate 
the code duplications found by the tool in the IIO subsystems and the AMD Display driver and 
pass the process of sending the mitigations as PATCH.

\section{Evaluation of the Tool by the BigCloneBench dataset}

\label{sec:metbig}

The BigCloneBench dataset \citep{bigclonebench} is a Java code collection containing pairs of code duplications categorized by clone type, as defined in Section \ref{subsec:types}.
%
Following the methodology presented by \citep{tailor}, we sampled 20,000 pairs from each clone type, adding 20,000 non-duplicate pairs as negative samples. We applied the same sampling approach to our tool to ensure a fair evaluation.

Unlike traditional methods, our tool does not distinguish between clone types and identifies duplications at the function level rather than the file level, which is typical in state-of-the-art tools. Therefore, we adapted the metric calculation method for evaluation purposes. Specifically, we considered every pair of functions with a similarity metric equal to or greater than a threshold \(X\) as duplicates and marked the corresponding file pairs. A correctly identified duplication pair counts as accurate for its clone type, while an incorrectly inferred pair is considered incorrect across all clone types. With these definitions, we computed the Recall metric from Section \ref{subsec:codemethods} for each clone type.
%
To understand the impact of varying the similarity threshold, we evaluated our tool using different threshold values: 30\%, 40\%, 50\%, 60\%, 70\%, 80\%, 90\%, and 100\%. We then analyzed and discussed the results.

\section{Evaluation of the Tool by Empirical Analysis}

\label{sec:metemp}

Due to the limitations of the BigCloneBench dataset, as noted by \cite{bigfail}, and its lack of suitability 
for our context (Java dataset vs. C-based Linux kernel), we complemented the BigCloneBench evaluation with 
an empirical study of the tool inferred duplications.

Given the vast number of lines and components in the Linux kernel, analyzing every duplication is impractical. We focused our scope on the AMD Display driver, which, as shown in Figure \ref{fig:relatory_ex}, contains over 20,000 lines of duplicated code at a similarity metric of 100\%.

In this empirical analysis, we randomly sampled function pairs identified by the tool as duplicates. For each similarity threshold \(X\) (30\%, 40\%, 50\%, 60\%, 70\%, 80\%, 90\%, and 100\%), we randomly selected ten function pairs with a similarity close to \(X\), allowing for a 1\% deviation. We implemented the random selection using the tool's source code.

Appendix \ref{app:emp} lists the randomly selected function pairs. We empirically evaluated these pairs to determine if they were duplications, comparing these results with the literature-based method to assess our tool's effectiveness and understand the impact of the minimum similarity parameter.

\section{Ethnographic Study}

\label{sec:meteth}

Ethnography is a recognized qualitative empirical method for understanding people, 
their cultures, and their work practices \citep{bookethno}. It allows researchers 
to gain insights into values, beliefs, and ideas from the perspective of community 
members \citep{ethnosoft}. In our research, refactoring code duplications without 
community interaction would prevent us from understanding whether Linux maintainers 
would accept our approach or identify their concerns. Thus, we will use ethnography, 
positioning ourselves and students as Linux kernel contributors focused on removing 
identified  code duplications.

This section outlines our approach to mitigating code duplication in the Linux kernel 
and describes the ethnographic methodology for engaging with the Linux community. 
Firstly, we present the participant observation research method explored by the author. 
Then, we present the methodology of the experiment done with the students.

\subsection{Participant observation method}
\label{subsec:partmethod}

Given the tool evaluations demonstrating the tool's capabilities of finding duplications, 
we proceeded to understand if the duplications found can be mitigated and the kernel 
subsystem's maintainers' opinions about the duplications and their respective mitigations. 
To accomplish this objective, we initially proposed a participant-observation experiment, 
where we, people who have never contributed to the Linux kernel, act as contributors to 
the AMD Display driver and collect artifacts of our experience in the process.

As a first step of this experiment, we executed the tool in the AMD Display driver codebase. 
We manually analyzed the biggest duplications regarding the number of lines and raised them 
to two duplicated function pairs that we judged interesting to try to mitigate. We have 
chosen to approach the biggest duplications as proof of concept so that we can make 
significant impacts. The two duplicated function pairs chosen are:

\begin{itemize}
\item \textbf{Duplicated function pair 1}
\begin{itemize}
\item Function named \textit{offset\_to\_id} in file \textit{dc/gpio/dcn32/hw\_translate\_dcn32.c}.
\item Function \textit{offset\_to\_id} in file \textit{dc/gpio/dcn315/hw\_translate\_dcn315.c}.
\end{itemize}
\item \textbf{Duplicated function pair 2}
%\begin{itemize}
\item Function named \textit{phy\_id\_to\_atom} in file \textit{dc/bios/dce110/command\_table\_helper\_dce110.c}.
\item Function named \textit{phy\_id\_to\_atom} in file \textit{dc/bios/dce60/command\_table\_helper\_dce60.c}.
%\end{itemize}
\end{itemize}

Given the two duplicated function pairs, we observed that the files in the functions 
contain multiple other duplications. Thus, we proposed a simple systematic approach 
to mitigate all the functions duplicated in the context, not just the duplicated 
function pairs. 
After we approached the duplications with the systematic approach, for the duplicated 
function pairs that had successfully mitigated the duplication, we sent patches to the 
AMD Display driver email list to validate the mitigations and see the maintainer's opinion 
while documenting the process and our impressions.
The systematic approach is defined below.

\subsubsection{Systematic Approach to Mitigate Code Duplication}
\label{subsubsec:systematic}

In C programming, communication between source files is achieved by creating header files that specify 
libraries \citep{Cbook}. Since the Linux kernel is primarily written in C, we eliminated code duplications 
by consolidating duplicated code into a single library, which would replace instances of 
duplication across the codebase.

For each duplicated function pair, for each function in the pair, we used the \textit{function information} functionality 
(described in Section \ref{subsec:functioncommand}) to locate all duplications of the function in the codebase, 
as it may have been duplicated more times than the two occurrences of the duplicated function pair. 
This strategy resulted in a collection of duplicated functions and corresponding code files.

To identify shared code more effectively, we extended this approach to search for other common 
functions across all collection files. We then applied specific refactoring methods to each shared 
function. If the functions were identical across files, we removed the duplicates and created a single 
function in the library. If modifications existed, we applied case-specific refactoring.

\subsection{Study with University Students}

Based on our initial participant observation, we decided to expand the experiment by proposing 
to students how to mitigate duplication found by the tool and send patches to the kernel community.

In the Free Software Development course at the University of São Paulo, taught by the advisor of 
this work in the first semester of 2025, we proposed to the students the possibility of mitigating 
duplications in the IIO subsystem and the AMD display driver found with the ArKanjo tool. 
The course had 26 students, of whom X were undergraduate students and Y were graduate students.

In the course, the students are asked to form groups of one, two, or three students and contribute 
to free software together. To assist the students in this task, the professor and teaching assistants 
prepare multiple alternatives for contributing to free software projects, including simple contributions 
such as updating documentation or code style fixes. In this context, we have proposed two options 
for the students related to the tool.

For the first alternative, one of the teaching assistants used the ArKanjo tool in the IIO subsystem 
and listed interesting duplicated function pairs that the groups could mitigate this specific 
duplication pair. It is up to the students to refactor the code to mitigate the duplications and 
pass through the process of sending patches to the IIO maintainers for review. For the second 
alternative, we propose a similar experience to the participant observation method, where the 
students pass through the whole process of executing the ArKanjo tool in the AMD display driver, 
analyze the duplications found, create a patch to mitigate a duplication, and send the patch to the 
AMD display driver’s maintainers.

From the students enrolled in the course, 25 students opted for the first alternative, and one 
student opted for the second alternative, forming 11 groups working on the first alternative 
and 1 group working on the second alternative. It is worth highlighting that none of the students 
who chose to mitigate duplications had contributed to the Linux kernel before, meaning that the 
patch to mitigate the duplications was the first tentative to contribute to the kernel as newcomers.

The groups have been requesting that we document the experience and approaches to refactoring the 
duplications and the experience of submitting a patch to the Linux kernel in blogs. We analyzed 
these blogs to understand the patterns of refactoring used for mitigating the duplications, the 
experience of sending the patches, and the opinions of the subsystem maintainers on the mitigations.

The teaching assistant and the student who executed the ArKanjo tool to explore duplications were 
asked to answer some questions related to their experience using the tool and what could be improved. 
The survey with the users' answers can be found in Appendix \ref{app:survey}.

In the second stage of the Free Software Development course, we propose that students contribute 
directly to the ArKanjo tool development without help from the tool creator to verify whether the 
tool can be free software to which multiple people can contribute. The students approached the tool’s 
issue, and to our knowledge, they pointed to us in the survey feedback. Three students forming two 
groups worked in direct contributions, which was not a very expressive number compared with the 
initial experiment.
