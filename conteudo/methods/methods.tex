\todo[inline]{Depois, veja melhor o tempo verbal ... tem coisas que coloquei no presente, porque ainda está fazendo ou irá fazer; tem outras coisas já planejadas e feitas que o texto ficou no passado ... veja se está fluindo bem e como deixar mais uniforme o tempo verbal aqui e nas outras partes do texto, tentando deixar claro que já está feito e o que será feito depois}

We employ a multi-method research approach, using different methodologies to achieve our goals. Our first choice is to conduct an ethnographic study using a participant-observation methodology. In this study, we aim to mitigate a subset of duplicated functions by refactoring them systematically while interacting with the Linux community to gather feedback on our mitigation efforts. To address our main objective of mitigating code duplication in the Linux kernel, we first needed the ability to identify these duplications accurately. Thus, we developed a tool and assessed its capability to detect code duplications in the Linux kernel.

We apply two independent methods and perform triangulation on the results. The first method evaluates the proposed tool using an approach from the literature, comparing it against the BigCloneBench dataset \citep{bigclonebench}. The second method is an empirical analysis of a subset of functions within the AMD Display driver that our tool identified as duplicates. After completing these two methods, we will triangulate the findings to analyze the results comprehensively. We chose triangulation because relying solely on the literature-based method has limitations and may not be entirely reliable \citep{bigfail, litreview}.

\section{Evaluation of the Tool by Formal Literature}

\label{sec:metbig}

The BigCloneBench dataset \citep{bigclonebench} is a Java code collection containing pairs of code duplications categorized by clone type, as defined in Section \ref{subsec:types}.
%
Following the methodology presented by \citep{tailor}, we sampled 20,000 pairs from each clone type, adding 20,000 non-duplicate pairs as negative samples. We applied the same sampling approach to our tool to ensure a fair evaluation.

Unlike traditional methods, our tool does not distinguish between clone types and identifies duplications at the function level rather than the file level, which is typical in state-of-the-art tools. Therefore, we adapted the metric calculation method for evaluation purposes. Specifically, we considered every pair of functions with a similarity metric equal to or greater than a threshold \(X\) as duplicates and marked the corresponding file pairs. A correctly identified duplication pair counts as accurate for its clone type, while an incorrectly inferred pair is considered incorrect across all clone types. With these definitions, we computed the Recall metric from Section \ref{subsec:codemethods} for each clone type.
%
To understand the impact of varying the similarity threshold, we evaluated our tool using different threshold values: 30\%, 40\%, 50\%, 60\%, 70\%, 80\%, 90\%, and 100\%. We then analyzed and discussed the results.

\section{Evaluation of the Tool by Empirical Analysis}

\label{sec:metemp}

Due to the limitations of the BigCloneBench dataset, as noted by \cite{bigfail}, and its lack of suitability for our context (Java dataset vs. C-based Linux kernel), we complemented the formal evaluation with an empirical study of the tool inferred duplications.
%
Given the vast number of lines and components in the Linux kernel, analyzing every duplication is impractical. We focused our scope on the AMD Display driver, which, as shown in Figure \ref{fig:relatory_ex}, contains over 20,000 lines of duplicated code at a similarity metric of 100\%.

In this empirical analysis, we randomly sampled function pairs identified by the tool as duplicates. For each similarity threshold \(X\) (30\%, 40\%, 50\%, 60\%, 70\%, 80\%, 90\%, and 100\%), we randomly selected ten function pairs with a similarity close to \(X\), allowing for a 1\% deviation. We implemented the random selection using the tool's source code.

Appendix \ref{app:emp} lists the randomly selected function pairs. We empirically evaluated these pairs to determine if they were duplications, comparing these results with the literature-based method to assess our tool's effectiveness and understand the impact of the minimum similarity parameter.

\section{Ethnographic Study}

\label{sec:meteth}

Ethnography is a recognized qualitative empirical method for understanding people, their cultures, and their work practices \citep{bookethno}. It allows researchers to gain insights into values, beliefs, and ideas from the perspective of community members \citep{ethnosoft}. In our research, refactoring code duplications without community interaction would prevent us from understanding whether Linux maintainers would accept our approach or identify their concerns. Thus, we used ethnography, positioning ourselves as Linux kernel contributors focused on removing identified code duplications.

This section outlines our systematic approach for mitigating code duplication in the Linux kernel, details the methodology for selecting duplicated functions, and describes the ethnographic methodology for engaging with the Linux community.

\subsection{Systematic Approach to Mitigate Code Duplication}
\label{subsec:pipeline}

In C programming, communication between source files is achieved by creating header files that specify libraries \citep{Cbook}. Since the Linux kernel is primarily written in C, we eliminated code duplications by consolidating duplicated code into a single library, which would replace instances of duplication across the codebase.
%
For each duplicated function pair, we used the \textit{function information} functionality (described in Section \ref{subsec:functioncommand}) to locate all instances of the function in the code. This strategy resulted in a collection of duplicated functions and corresponding code files.

To identify shared code more effectively, we extended this approach to search for other common functions across all collection files. We then applied specific refactoring methods to each shared function. If the functions were identical across files, we removed the duplicates and created a single function in the library. If modifications existed, we applied case-specific refactoring.

Successful mitigations were submitted to the Linux community to gauge their feedback. If unsuccessful, we categorized the cases: some required specialist decisions beyond our expertise, and others were potentially non-refactorable. All mitigations were documented for research transparency at \url{https://github.com/LipArcanjo/master-artifacts}.

\subsection{Selection of Duplicated Functions}

As with the empirical analysis, evaluating every code duplication in the Linux kernel is impractical. Thus, we limited the experiment to the AMD Display driver and used random sampling of duplicated functions inferred by our tool to initiate the mitigation approach in Section \ref{subsec:pipeline}. We chose a sample size of 30 duplicated functions, as studies indicate this size provides statistical significance \citep{sample1, sample2}.

For random selection, we restricted eligibility to function pairs with a similarity metric of 100\%. With this criterion, we randomly chose functions from eligible pairs uniformly.

\subsection{Ethnographic Methodology}

We conduct ethnographic research upon completing a code duplication mitigation following the systematic approach in Section \ref{subsec:pipeline}. We adopt a participant-observation methodology, contributing to the Linux kernel for the AMD Display driver and interacting with maintainers to incorporate our mitigations.

Linux kernel communication occurs via public mailing lists, where discussions are accessible to all. We collect data artifacts from the AMD Display driver mailing list, which records community feedback on our mitigation efforts, providing valuable insights into the proposed approach effectiveness.

Given our research timeline, we do not expect continuous interactions with the Linux community throughout all mitigations, as response time varies. Thus, we committed to applying the participant-observation approach for three to five successful mitigations and using these for our final discussion.
