\en 

\todo[inline]{Juntei os capítulos; veja se consegue colocar algo aqui no início; não sei se final remarks é o melhor nome para a segunda secao aqui}

To finalize this report, we present the current state of our research. As of the 
time of writing, we have finished \textbf{Phase I} of this research, 
which includes the implementation of 
the proposed tool and the evaluation of the tool through both formal literature 
and empirical analyses. This chapter begins with the presentation of the results 
from the tool evaluation. Next, we present a preliminary experiment we are currently 
conducting to assess the viability of our Ethnographic Study. Finally, we outline 
the next steps in this research.

\section{Tool Evaluation}
\label{sec:eval}

In this section, we present the test results of our tool performance, following the methodology described in Chapter \ref{cha:method}. First, we provide the results obtained by our tool on the BigCloneBench dataset \citep{bigclonebench}, which serves as a standard reference in the literature \citep{litreview}. Second, we discuss the results from our empirical analysis. Finally, we triangulate the data, analyzing our findings regarding the tool performance.

\subsection{Tool Results on the BigCloneBench Dataset}

\input{conteudo/eval/table_bigclone}

As described in Section \ref{sec:metbig}, for evaluating our tool on the BigCloneBench dataset \citep{bigclonebench}, we define that every pair of functions with a similarity metric equal to or greater than a threshold \( X \) is considered a duplication. The pair of code files containing these functions are then labeled as duplications. If our tool correctly identifies a code duplication pair, it is counted as correct for the respective code duplication type. If it infers a duplication pair incorrectly, it is counted as incorrect for all code duplication types. Table \ref{tab:bigclone} shows the results obtained by our tool on the BigCloneBench dataset \citep{bigclonebench}.

The results in Table \ref{tab:bigclone} indicate that our tool is conservative in its predictions. No negative sample was inferred as a duplication, even with a minimum similarity threshold of 30\%. However, it performs poorly identifying complex duplication types, such as Medium Type-3 and Weak Type-3/Type-4. As discussed in Chapter \ref{cha:tool}, if improved performance on these complex types is needed, our tool allows replacing the current duplication detection method due to its modular design.

\subsection{Tool Results on Empirical Analysis}

As outlined in Section \ref{sec:metemp}, for the empirical evaluation of our tool, we randomly selected ten function pairs across various similarity metric ranges computed by our tool. We then examined each pair to determine whether they were duplications and, if so, categorized the duplication type. Table \ref{tab:emp} presents the results from our empirical analysis.

\input{conteudo/eval/table_empirical}

The results in Table \ref{tab:emp} show that as the similarity metric decreases, 
there is a notable increase in function pairs that are not duplications within 
the Linux kernel. For similarity ranges of 79\%-81\% or higher, there is a high 
percentage of code duplications. At 69\%-71\% similarity, the duplication rate 
drops to 50\%. At 59\%-61\%, it further decreases to 20\%, and below 49\%-51\%, 
no code duplications were found.

\subsection{Triangulation of Results}

The BigCloneBench results and our empirical methods indicate that our tool performs well in detecting Type-1 and Type-2 code clone duplications but struggles with more complex types. A similarity threshold between 80\% and 100\% yields favorable results in both methods.

However, there is a discrepancy in the detection of non-duplication pairs between the two methods. In the BigCloneBench evaluation, we found no false positives (negative samples inferred as duplications), while the empirical analysis revealed a higher percentage of false positives. We propose two potential explanations for this discrepancy.

The first reason may be the limitations and known issues with BigCloneBench, as highlighted by Krinke and Ragkhitwetsagul \citep{bigfail}. The second reason could relate to the nature of the AMD Display driver, where each code artifact shares semantic meaning. In contrast, BigCloneBench comprises self-contained code artifacts without shared semantics. Given that our tool relies on a text-based code clone detection approach, it may naturally perform worse on the AMD Display driver than on the BigCloneBench dataset. We cannot determine if state-of-the-art code duplication detection tools face similar issues obscured by evaluations limited to the BigCloneBench dataset.
