On this Chapter, we exposes our experiment results bout the proposed tool performance following
the methology described in Chapter \ref{cha:method}. Firstly, we shows the results obtained by our tool
in the BigCloneBench dataset\citep{bigclonebench}, which is the standard form of the formal literature 
\citep{litreview}. Secondly, we shows the results obtained by our tool on the empirical analyses. 
Finaly, we do a triangulation of data analysing our finds on the tool performance.

\subsection{Tool results on the BigCloneBench dataset}

\input{conteudo/eval/table_bigclone}

As described in section \ref{sec:metbig}. For the evaluation of our proposed tool in the
BigCLoneBench dataset  \citep{bigclonebench}, we define that every pair of functions in which the 
similarity metric is equal or superior to a threshold X will be considered a duplication, and the 
pair of code files in which the functions exist are labeled as duplication. If, on this method, 
a code duplication pair is correctly identified, it will count as correct to its code duplication 
types. If an incorrect duplication pair is inferred as a duplication, it will count as incorrect 
to all code duplication types. Table \ref{tab:bigclone} shows the results obtained by our tool in the 
BigCloneBench dataset \citep{bigclonebench}.

The results presented in Table \ref{tab:bigclone} indicate that our tool is a pessimist. There 
is no negative sample inferred as duplication even for a minimum similarity threshold of 30\%, 
on the other hand, it is bad for identifying code duplications of complex code duplication types, 
that is Medium Type-3 and Weak Type-3/Type-4. As mentioned in Chapter \ref{cha:tool}, if a 
better result is required on these complex code duplication types, it is possible to change 
the code duplication detection method, as it is designed to be a replaceable component in our 
tool, given the limitations.

\subsection{Tool results on the empirical analyses}

As described in section \ref{sec:metemp} For the evaluation of our proposed tool by our empirical 
analyses, we randomly selected ten function pairs for a variation of ranges of the similarity 
metric computed by our tool. After that, we analyzed if they were duplications, and if yes, 
we judged the duplication type. Table \ref{tab:emp} presents the results obtained by our tool 
in the empirical analyses.

\input{conteudo/eval/table_empirical}

The results presented in Table \ref{tab:emp} indicate that as the similarity metric reduces, 
there is a significant increase in function pairs that are not duplications in the Linux kernel. 
For the ranges 79\%-81\% of similarity or greater, we have a high percentage of code duplications. 
With 69\%-71\% of similarity, the rate goes down to 50\%. With 59\%-61\%, the rate decreases to 20\%.
With 49\%-51\% similarity or lower, we did not find any code duplications.

\subsection{Discussion}
