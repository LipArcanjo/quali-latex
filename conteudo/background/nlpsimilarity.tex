\en

\section{Text similarity detection methods}
\label{sec:similarity}.

The standard methodology for text similarity detection are based on building vector embeddings of
the texts and apply a distance function in these embeddings to measure the similarity between them. 
There are multiple methods for building vector embeddings, such as bag-of-words, TF-IDF, LSI, Word2Vec \citep{gensimlivro}. 
The recent works related to build vector embeddings uses Largue Language Models (LLM), for example the usage of BERT, ELMo 
and GPT \citep{llmsimilar}. The most popular distance function used by the literature are the cosine similarity 
defined by the formula \citep{cosineref}:

$$\text{cosine similarity} = SC(A,B) = \frac{ A \cdot B}{ \lVert A \rVert \lVert B \rVert }$$


Where $A$ and $B$ are the vector embeddings of the two texts. One of the advantages of the cosine similarity is that $SC(A,B)$ 
is a number between $0$ and $1$, where $SC(A,B)$ values more close to the number $1$ means that two vectors are more similar. 

It is a famous fact that LLM have high computational cost, one example of such is a experiment done by Reimers and Gurevych for 
finding the most similar pair in a collection of 10,000 sentences, that required about 65 hours with BERT \citep{bertsimilar}. This 
is a similar magnitude of comparation pairs we want to do through our work and for this reason we chose to not use this kind of 
vector embedding method, but it is a valid solution for smaller codebases.

\subsection{Gensim}

Gensim is a open source python library \citep{gensim} which the owners declare themselves as "The fastest library 
for training of vector embeddings - Python or otherwise. The core algorithms in Gensim use battle-hardened, 
highly optimized and parallelized C routines." \citep{gensimsite}. For the velocity of gensim we chose to use this library
for doing our vector embeddings. 

In this work we will not explore more alternatives for vector embeddings. We do not want to optimize the computational 
cost or the code clone detection to the state-of-art as they are not the main objectives of our research.


