\en

\section{Text similarity detection methods}

The standard methodology for text similarity detection are based on building vector embeddings of
the texts and apply a distance function in these embeddings to measure the similarity between them. 
There are multiple methods for building vector embeddings, such as bag-of-words, TF-IDF, LSI, Word2Vec \citep{gensimlivro}. 
The recent works related to build vector embeddings uses Largue Language Models (LLM), for example the usage of BERT, ELMo 
and GPT \citep{llmsimilar}. The most popular distance function used by the literature are the cosine similarity 
defined by the formula \citep{cosineref}:

$$\text{cosine similarity} = SC(A,B) = \frac{ A \cdot B}{ \lVert A \rVert \lVert B \rVert }$$


Where $A$ and $B$ are the vector embeddings of the two texts. One of the advantages of the cosine similarity is that $SC(A,B)$ 
is a number between $0$ and $1$, where $SC(A,B)$ values more close to the number $1$ means that two vectors are more similar. 

It is a famous fact that LLM have high computational cost, one example of such is a experiment done by Reimers and Gurevych for 
finding the most similar pair in a collection of 10,000 sentences, that required about 65 hours with BERT \citep{bertsimilar}. This 
is a similar magnitude of comparation pairs we want to do through our work and for this reason we chose to not use this kind of 
vector embedding method, but it is a valid solution for smaller codebases.
